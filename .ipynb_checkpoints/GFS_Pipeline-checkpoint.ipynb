{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55c3d7f-0caf-4b4d-93ac-7a33609a3c78",
   "metadata": {},
   "source": [
    "# Ingest NCEP GFS 0.25 Degree Data for 6 hour forecasts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13cbc7-04cb-48bf-9f35-b9670e4446c3",
   "metadata": {},
   "source": [
    "#### 1.) Conda package installations to environment and importing appropriate libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ "
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge gdal\n",
    "# conda install -c conda-forge geopandas\n",
    "# conda install -c conda-forge earthpy\n",
    "# conda install -c conda-forge cloudpathlib\n",
    "# conda install -c conda-forge pyhdf\n",
    "# conda install -c anaconda basemap\n",
    "\n",
    "#conda install -c conda-forge ipywidgets\n",
    "#conda install -c conda-forge cartopy\n",
    "\n",
    "## For IO dependencies in xarray \n",
    "!conda install -c conda-forge xarray dask netCDF4 bottleneck\n",
    "#conda install -c conda-forge cfgrib\n",
    "#conda install -c conda-forge pygrib\n",
    "#conda install -c yt87 pywgrib2_xr\n",
    "\n",
    "## For writing / reading parquet files \n",
    "#conda install -c conda-forge pyarrow\n",
    "\n",
    "#For s3 \n",
    "#conda install -c conda-forge boto3\n",
    "\n",
    "#For timing \n",
    "#conda install -c conda-forge profilehooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e56688-186f-4277-b544-9f03b27ceb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3b12cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7520/802162718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#GFS data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;31m# used for reading the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxarray_extras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m \u001b[0;31m# used for writing data to csv format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpygrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xarray'"
     ]
    }
   ],
   "source": [
    "#Import Packages. \n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "#from shapely.geometry import mapping, box\n",
    "#import geopandas as gpd\n",
    "# import earthpy as et\n",
    "# import earthpy.spatial as es\n",
    "# import earthpy.plot as ep\n",
    "# from osgeo import gdal\n",
    "import pandas as pd\n",
    "\n",
    "#GFS data\n",
    "import xarray # used for reading the data.\n",
    "import xarray_extras.csv # used for writing data to csv format. \n",
    "import pygrib\n",
    "import xarray # used for reading the data.\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt # used to plot the data.\n",
    "import ipywidgets as widgets # For ease in selecting variables.\n",
    "import cartopy.crs as ccrs # Used to georeference data.\n",
    "\n",
    "#For writing to s3\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "#For timing function\n",
    "from profilehooks import profile\n",
    "\n",
    "#For multiprocessing of function\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# #from cloudpathlib import S3Path, S3Client\n",
    "# from pyhdf.SD import SD, SDC\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad8770-f561-455b-b040-ad494e3e865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of CPUs\n",
    "print(\"Number of cpu : \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4921a1-3e5c-4c0c-aded-61fd426b0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfdfaa-0a14-4dd2-a9de-43a5f2034b28",
   "metadata": {},
   "source": [
    "#### Process grid_metadata.csv to get updated lat, lon bounds for a 5km x 5km labeled grid. Pull in grid ID for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f378b-a271-4c2d-9c33-d6261c66e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grids = pd.read_csv('../capstone/grid_metadata.csv')\n",
    "\n",
    "final_grid_ids = []\n",
    "final_min_lats = []\n",
    "final_max_lats = []\n",
    "final_min_lons = []\n",
    "final_max_lons = []\n",
    "\n",
    "#Note order is lon, lat for each of the comma separated values\n",
    "for index, row in df_grids.iterrows(): \n",
    "    lons = []\n",
    "    lats = []\n",
    "    grid_id = row['grid_id']\n",
    "    nums = row['wkt'][10:-2]\n",
    "    nums = nums.replace(',','')\n",
    "    pairs = nums.split(' ')\n",
    "    for i in range(10): \n",
    "        if i % 2 == 0: \n",
    "            lons.append(pairs[i])\n",
    "        else: \n",
    "            lats.append(pairs[i])\n",
    "    \n",
    "    # Adding +/- 0.125 guarantees we get at least 1, 0.25 degree forecast in the appropriate area (given GFS scale is larger than Sat. scale). \n",
    "    min_lat = float(min(lats)) - 0.15\n",
    "    max_lat = float(max(lats)) + 0.15\n",
    "    # Match our 0 to 360 longitudes in GFS data, vs the -180 to +180 longitudes here. \n",
    "    min_lon = float(min(lons)) + 180.00 - 0.15\n",
    "    max_lon = float(max(lons)) + 180.00 + 0.15\n",
    "    \n",
    "    #Now append the appropriate scraped data to our lists to put into a dataframe. \n",
    "    final_grid_ids.append(grid_id)\n",
    "    final_min_lats.append(min_lat)\n",
    "    final_max_lats.append(max_lat)\n",
    "    final_min_lons.append(min_lon)\n",
    "    final_max_lons.append(max_lon) \n",
    "                        \n",
    "    \n",
    "    \n",
    "df_grids_clean = pd.DataFrame(columns = ['grid_id', 'min_lat', 'max_lat', 'min_lon', 'max_lon'])\n",
    "\n",
    "\n",
    "df_grids_clean['grid_id'] = final_grid_ids \n",
    "df_grids_clean['min_lat'] = final_min_lats\n",
    "df_grids_clean['max_lat'] = final_max_lats\n",
    "df_grids_clean['min_lon'] = final_min_lons\n",
    "df_grids_clean['max_lon'] = final_max_lons\n",
    "\n",
    "    \n",
    "df_grids_clean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195e556-2572-40b7-abe3-3320c9a21860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define lat/lon bounds of our regions of interest. \n",
    "#Note: We must convert the original lon bounds of -180, 180 --> 0, 360 to match the GFS data format. \n",
    "\n",
    "#Los Angeles\n",
    "la_min_lat = 30.01\n",
    "la_max_lat = 40.00\n",
    "la_min_lon = 49.46\n",
    "la_max_lon = 76.06\n",
    "la_bounds = [la_min_lat, la_max_lat, la_min_lon, la_max_lon]\n",
    "\n",
    "#Tapei\n",
    "tp_min_lat = 20.01\n",
    "tp_max_lat = 30.00\n",
    "tp_min_lon = 297.07\n",
    "tp_max_lon = 318.55\n",
    "tp_bounds = [tp_min_lat, tp_max_lat, tp_min_lon, tp_max_lon]\n",
    "\n",
    "#Delhi\n",
    "dl_min_lat = 20.01\n",
    "dl_max_lat = 30.00\n",
    "dl_min_lon = 243.85\n",
    "dl_max_lon = 260.82\n",
    "dl_bounds = [dl_min_lat, dl_max_lat, dl_min_lon, dl_max_lon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4347c97-44da-4fe8-874e-4ac6f8a3d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter by appropriate lat/lon bounds\n",
    "def subset_dataset(dataset, min_lat, max_lat, min_lon, max_lon): \n",
    "    '''Takes a dataset and bounding coordinates and returns a filtered subset for the region of interest'''\n",
    "    mask_lat = np.logical_and(dataset.coords['latitude'] >= min_lat, dataset.coords['latitude'] <= max_lat)\n",
    "    mask_lon = np.logical_and(dataset.coords['longitude'] >= min_lon, dataset.coords['longitude'] <= max_lon)\n",
    "    ds_filt = dataset.where(mask_lat & mask_lon, drop = True)\n",
    "    return ds_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653bcc3-4530-4967-aad6-23884387ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_df(dataset, level, var_name, grid_id, time): \n",
    "    '''Convert xarray Dataset to Dataframe, Drop Unecessary Columns, Add Grid ID, and Rename Columns'''\n",
    "    df = dataset.to_dataframe(name = var_name)\n",
    "    df = df.drop(columns = [level, 'time', 'step', 'valid_time'])\n",
    "    df.insert(0, 'grid_id', grid_id)\n",
    "    df = df.rename(columns = {\"t\" : \"t_surface\" + time, \"hpbl\" : \"pbl_surface\" + time, \"landn\" : \"landn_surface\" + time, \"hindex\" : \"hindex_surface\" + time, \"gust\" : \"gust_surface\" + time, \n",
    "                              \"r\" : \"r_atmosphere\" + time, \"pwat\": \"pwat_atmosphere\" + time, \n",
    "                              \"u\" : \"u_pbl\" + time, \"v\": \"v_pbl\" + time, \"VRATE\" : \"vrate_pbl\" + time})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081fec6-d1fb-4338-81d7-158ed7a71256",
   "metadata": {},
   "source": [
    "#### 2.) Download data from NCAR servers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f2bb0-8abe-474c-b7a9-e84985ae2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## First, we need to authenticate\n",
    "try:\n",
    "    import getpass\n",
    "    input = getpass.getpass\n",
    "except:\n",
    "    try:\n",
    "        input = raw_input\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d56a8-e57c-4377-a915-e87c42e79cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, we need your password.\n",
    "pswd = input('password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d97c9-2936-4b01-ba49-b6a11f271fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'email' : 'jericojohns@berkeley.edu', 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1ff73-c160-4e64-8d30-d2c70a47a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = requests.post(login_url, data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d207e-e3ef-4d9b-b6c2-fbf349c19374",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspath = 'https://rda.ucar.edu/data/ds084.1/'\n",
    "save_dir = '/local/train/GFS/'\n",
    "filelist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbb568-038a-46fa-a68c-75f489ef8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For identifying variable names and layers\n",
    "file = '2018' + '/' + '2018' + '02' + '01' + '/gfs.0p25.' + '2018' + '02' + '01' + '00' + '.f006.grib2'\n",
    "filename = dspath + file\n",
    "outfile = save_dir + os.path.basename(filename) \n",
    "print('Downloading', file)\n",
    "req = requests.get(filename, cookies = ret.cookies, allow_redirects=True)\n",
    "open(outfile, 'wb').write(req.content)\n",
    "filelist_arr = [save_dir + os.path.basename(file)]\n",
    "selected_file = widgets.Dropdown(options=filelist_arr, description='data file')\n",
    "display(selected_file)\n",
    "\n",
    "#Now we use xarray to open the file by the type_of_level we are interested in \n",
    "type_of_level1 = 'planetaryBoundaryLayer' # for Temperature and Planetary Boundary Layer Height\n",
    "ds_level_surface = xarray.open_dataset(selected_file.value, filter_by_keys={'typeOfLevel': type_of_level1}, engine=\"cfgrib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6bfc3-f3e2-4faa-9a0f-98da34a9b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray.set_options(display_expand_attrs = True, display_expand_data_vars = True, display_width = 200, display_max_rows = 200)\n",
    "print(ds_level_surface.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470ac63-2aaa-4842-8a3e-e4d1f19d1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gfs_pipeline(years, months, days, times): \n",
    "    '''Pipeline the GFS data for specified times and specified variables, at the specified levels.\n",
    "       Output will be a parquet file with 1 row per unique lat/lon combination (within regions of interest). \n",
    "       Forecast times will be added column-wise so that there are 4 forecasts per variable per day per row.'''\n",
    "    for year in years: \n",
    "        for month in months: \n",
    "            for day in days: \n",
    "                final_df = pd.DataFrame()\n",
    "                for time in times:\n",
    "                    #In case file doesn't exist for a given date / time, continue to next day/time. \n",
    "                    try: \n",
    "                        #Download .glib2 file (temporarily) to scrape the desired fields. We will delete after use. \n",
    "                        file = year + '/' + year + month + day + '/gfs.0p25.' + year + month + day + time + '.f006.grib2'\n",
    "                        filename = dspath + file\n",
    "                        outfile = save_dir + os.path.basename(filename) \n",
    "                        print('Downloading', file)\n",
    "                        req = requests.get(filename, cookies = ret.cookies, allow_redirects=True)\n",
    "                        open(outfile, 'wb').write(req.content)\n",
    "                        filelist_arr = [save_dir + os.path.basename(file)]\n",
    "                        selected_file = widgets.Dropdown(options=filelist_arr, description='data file')\n",
    "                        display(selected_file)\n",
    "\n",
    "                        #Now we use xarray to open the file by the type_of_level we are interested in \n",
    "                        type_of_level1 = 'surface' # for Temperature and Planetary Boundary Layer Height, land cover, haines index, max surface wind speed \n",
    "                        type_of_level2 = 'atmosphereSingleLayer' # for Relative Humidity and preciptable water\n",
    "                        type_of_level3 = 'planetaryBoundaryLayer' # For U and V wind, and ventilation rate\n",
    "                        ds_level_surface = xarray.open_dataset(selected_file.value, filter_by_keys={'typeOfLevel': type_of_level1}, engine=\"cfgrib\")\n",
    "                        ds_level_atmosphere = xarray.open_dataset(selected_file.value, filter_by_keys={'typeOfLevel': type_of_level2}, engine=\"cfgrib\")\n",
    "                        ds_level_pbl = xarray.open_dataset(selected_file.value, filter_by_keys={'typeOfLevel': type_of_level3}, engine=\"cfgrib\")\n",
    "\n",
    "                        #Define surface variable names\n",
    "                        var_t = 't' #Temperature (K) \n",
    "                        var_hpbl = 'hpbl' #Planetary Boundary Layer Height (m)\n",
    "                        var_landn = 'landn' #Land cover (0=sea, 1=land) (Proportion)\n",
    "                        var_hindex = 'hindex' #Haines index (fire index) \n",
    "                        var_gust = 'gust' #Wind speed (gust) (m s-1)\n",
    "\n",
    "                        #Define atmosphere single Layer variables\n",
    "                        var_r = 'r' #Relative Humidity %\n",
    "                        var_pwat = 'pwat' #Precipitable water (kg m-2)\n",
    "\n",
    "                        #Planetary Boundary Layer\n",
    "                        var_u = 'u' #u-component of wind (m s-1)\n",
    "                        var_v = 'v' #v-component of wind (m s-1)\n",
    "                        var_VRATE = 'VRATE' #Ventilation rate (m2 s-1)\n",
    "\n",
    "                        #Define filtered datasets (for each variable). \n",
    "                        #Surface \n",
    "                        ds_t = ds_level_surface[var_t] \n",
    "                        ds_hpbl = ds_level_surface[var_hpbl]\n",
    "                        ds_landn = ds_level_surface[var_landn]\n",
    "                        ds_hindex = ds_level_surface[var_hindex]\n",
    "                        ds_gust = ds_level_surface[var_gust]\n",
    "\n",
    "                        #Atmosphere\n",
    "                        ds_r = ds_level_atmosphere[var_r]\n",
    "                        ds_pwat = ds_level_atmosphere[var_pwat]\n",
    "\n",
    "                        #Planetary Boundary Layer\n",
    "                        ds_u = ds_level_pbl[var_u]\n",
    "                        ds_v = ds_level_pbl[var_v]\n",
    "                        ds_VRATE =ds_level_pbl[var_VRATE]\n",
    "\n",
    "\n",
    "                        #Initialize empty dataframe to append each regional dataframe to\n",
    "                        daily_df = pd.DataFrame()\n",
    "                        for index, row in df_grids_clean.iterrows(): \n",
    "                            grid_id = row['grid_id']\n",
    "                            min_lat = row['min_lat']\n",
    "                            max_lat = row['max_lat']\n",
    "                            min_lon = row['min_lon']\n",
    "                            max_lon = row['max_lon']\n",
    "\n",
    "                            #Filter to bounds of 5x5km regions of interest. \n",
    "                            #Surface\n",
    "                            ds_t_filt = subset_dataset(ds_t, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_hpbl_filt = subset_dataset(ds_hpbl, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_landn_filt = subset_dataset(ds_landn, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_hindex_filt = subset_dataset(ds_hindex, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_gust_filt = subset_dataset(ds_gust, min_lat, max_lat, min_lon, max_lon)\n",
    "\n",
    "                            #Atmosphere\n",
    "                            ds_r_filt = subset_dataset(ds_r, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_pwat_filt = subset_dataset(ds_pwat, min_lat, max_lat, min_lon, max_lon)\n",
    "\n",
    "                            #PBL\n",
    "                            ds_u_filt = subset_dataset(ds_u, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_v_filt = subset_dataset(ds_v, min_lat, max_lat, min_lon, max_lon)\n",
    "                            ds_VRATE_filt = subset_dataset(ds_VRATE, min_lat, max_lat, min_lon, max_lon)\n",
    "\n",
    "\n",
    "                            #Make sure we preserve the type of level (atmospheric) of the observation to preserve metadata within the variable names\n",
    "                            ##Surface\n",
    "                            df_t = dataset_to_df(ds_t_filt, 'surface', var_t, grid_id, time)\n",
    "                            df_hpbl = dataset_to_df(ds_hpbl_filt, 'surface', var_hpbl, grid_id, time)\n",
    "                            df_landn = dataset_to_df(ds_landn_filt, 'surface', var_landn, grid_id, time)\n",
    "                            df_hindex = dataset_to_df(ds_hindex_filt, 'surface', var_hindex, grid_id, time)\n",
    "                            df_gust = dataset_to_df(ds_gust_filt, 'surface', var_gust, grid_id, time)\n",
    "\n",
    "                            #Atmosphere\n",
    "                            df_r = dataset_to_df(ds_r_filt, 'atmosphereSingleLayer', var_r, grid_id, time)\n",
    "                            df_pwat = dataset_to_df(ds_pwat_filt, 'atmosphereSingleLayer', var_pwat, grid_id, time)\n",
    "\n",
    "                            #PBL\n",
    "                            df_u = dataset_to_df(ds_u_filt, 'planetaryBoundaryLayer', var_u, grid_id, time)\n",
    "                            df_v = dataset_to_df(ds_v_filt, 'planetaryBoundaryLayer', var_v, grid_id, time)\n",
    "                            df_VRATE = dataset_to_df(ds_VRATE_filt, 'planetaryBoundaryLayer', var_VRATE, grid_id, time)\n",
    "\n",
    "\n",
    "                            #Now join all fields into same df\n",
    "                            data_frames = [df_t, df_hpbl, df_landn, df_hindex, df_gust, df_r, df_pwat, df_u, df_v, df_VRATE]\n",
    "\n",
    "                            joined_df_current = reduce(lambda left,right: pd.merge(left,right, on=[\"latitude\", \"longitude\", \"grid_id\"],\n",
    "                                                how = 'left'), data_frames)\n",
    "\n",
    "                            # joined_df_current = pd.merge(df_t, df_pbl, on = [\"latitude\", \"longitude\", \"grid_id\"], how = \"left\")\n",
    "                            # joined_df_current = pd.merge(joined_df_current, df_r, on = [\"latitude\", \"longitude\", \"grid_id\"], how = \"left\")\n",
    "\n",
    "                            #Now concatenate current dataframe into final dataframe\n",
    "                            if daily_df.empty: \n",
    "                                daily_df = joined_df_current\n",
    "                            else: \n",
    "                                daily_df = pd.concat([daily_df, joined_df_current], axis = 0)\n",
    "\n",
    "                         #Now we delete the .grib2 file so as to save memory. Otherwise, we'd be storing ~1tb of data. \n",
    "                        if os.path.exists(outfile):\n",
    "                            os.remove(outfile)\n",
    "                        if os.path.exists(outfile + '.923a8.idx'):\n",
    "                            os.remove(outfile + '.923a8.idx')\n",
    "\n",
    "                        #Join the different forecast time dataframes together so that we have one column per forecast time. \n",
    "                        if final_df.empty: \n",
    "                            final_df = daily_df\n",
    "                        else: \n",
    "                            final_df = pd.merge(final_df, daily_df, on = [\"latitude\", \"longitude\", \"grid_id\"], how = \"left\")\n",
    "                    except: \n",
    "                        continue\n",
    "                try: \n",
    "                    #final_df = final_df.groupby(by = 'grid_id').mean()\n",
    "                    final_df.insert(0, 'date', year + '-' + month + '-' + day)\n",
    "                    #final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "                    #Convert to final_df to parquet, with the appropriate metadata in file name (will extract as field names later). \n",
    "                    out_parquet =  'gfs.0p25.' + year + month + day + '.f006.parquet'\n",
    "\n",
    "                    #For now just upload to s3\n",
    "                    filepath = '../train/GFS/parquet/' + out_parquet\n",
    "                    final_df.to_parquet(path = filepath, engine = 'pyarrow')\n",
    "\n",
    "                    #Put file in read mode so we can upload to s3 / Databricks storage bucket. \n",
    "                    with open(filepath, 'rb') as data:\n",
    "                        s3.upload_fileobj(data, 'capstone-particulate-storage', out_parquet)\n",
    "                        \n",
    "                except: \n",
    "                    continue\n",
    "\n",
    "                        \n",
    "    print(\"Pipeline complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb7c67-58e9-4a2a-b7aa-5d64facb4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Iterate through all file names. \n",
    "# for year in ['2018', '2019', '2020']: \n",
    "#     for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']: \n",
    "#         for day in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']: \n",
    "#             for time in ['00', '06', '12', '18']: \n",
    "\n",
    "\n",
    "years = ['2018']\n",
    "months = ['02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "days = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "times = ['00', '06', '12', '18']\n",
    "\n",
    "run_gfs_pipeline(years, months, days, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d9233-9e8f-416e-b228-65fbebcaca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Now try to parellize the for loops \n",
    "# years = ['2018']\n",
    "# months = ['02']\n",
    "# days = ['01']\n",
    "# times = ['00', '06', '12', '18']\n",
    "\n",
    "# args = (years, months, days, times)\n",
    "\n",
    "# def pool_handler(): \n",
    "#     p = Pool(4)\n",
    "#     p.map(run_gfs_pipeline, args)\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     pool_handler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944bb0e-36c3-4a62-8b5a-8f46b8500e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Multiprocessing\n",
    "# # #Now try to parellize the for loops \n",
    "# years = ['2018']\n",
    "# months = ['02']\n",
    "# days = ['01']\n",
    "# times = ['00', '06', '12', '18']\n",
    "\n",
    "# args = (years, months, days, times)\n",
    "\n",
    "# task = ProcessWithLogAndControls(target=run_gfs_pipeline, args=(args), name=\"GFS Pipeline\")\n",
    "# task.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738daaf-8dee-4995-971c-15c3500174fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1acb5e8-4a9f-4adf-9d53-2f6bddde9d5f",
   "metadata": {},
   "source": [
    " #### Now to download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e266283-82c6-4b8d-865c-d13e3f5f3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODOs: \n",
    "# Do this for each region and concatenate the 3 dataframes into one dataframe. (Do we want to add column with region labeled?). \n",
    "# Create strings for each possible filename (i.e. 01 through 31 for 01 through 12 months for 2018 to 2020 years). \n",
    "# Use Srishti's S3 bucket and add a test csv file to the bucket (so we don't have to store locally). \n",
    "# Pull file download, df creation, df to csv save to s3 (forecast time) and file deletion into one loop function (based on dates above). Quick exit if error bc date doesn't exist (i.e. 31).\n",
    "# Make sure we can pass tuples or some combination for level and variable name into function so that we can quickly change variables included. \n",
    "# Add a timeit call to understand how long it takes to run end-to-end pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae6ff3-ae96-45ae-b915-679bc86e9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parquet =  'gfs.0p25.' + '2018' + '02' + '01' + '.f006.parquet'\n",
    "filepath = '../train/GFS/parquet/' + out_parquet\n",
    "test_df = pd.read_parquet(filepath, engine='pyarrow')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a0cdd-0f87-44ae-9277-13a120da0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b44850-5446-4643-ae18-c83fc1c1796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "test_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb4405-dbd3-47fc-b479-9a51f7718bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array_data = io.BytesIO()\n",
    "pickle.dump(test_df, my_array_data)\n",
    "my_array_data.seek(0)\n",
    "s3.upload_fileobj(my_array_data, 'particulate-articulate-capstone','gfs_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638bf8c-2199-4e02-87ec-7ccc4a438ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to download \n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "s3.download_file('particulate-articulate-capstone', 'trial1maiac.pkl', 'trial1maiac.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bcd82-0f17-474c-8d98-645490fb3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2d9c6-e9b4-436b-8025-a2c4bd343221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle('../train/train/gfs.0p25.20180201.f006.parquet')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1b487-e8b1-4692-9d75-b61ad6165b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
